group,Skeleton/Structure - Check the box for each file that is included in the folder.,Skeleton/Structure Deductions - Check the box for each issue found.,Code Style - check the applicable boxes,"Reproducibility of report. You don't have to actually run anything, but is it clear to you, based on what is in the ""code"" folder, how all the figures/results in the report were produced? Would it be easy to reproduce them? If not, discuss why.

(Since I'm not asking you to run anything, I haven't provided any additional items students may have put in their ""data"" folders, but if e.g. the code refers to '../data/some-input-file.csv', please assume it exists)",How easy would it be for you to use their code to train models in the same way they did? Are the relevant scripts easy to use? Is the overall code structure made clear?,Readability of report (Was the narrative clear and easy to read? Or did you find it hard to follow? Any grammar mistakes?),Relevance of figures - excluding findings (were the figures relevant and discussed in the report?),"Quality of figures (Were the figures easy to understand? Were there captions? Were the axes labeled? Were they visually appealing? If not, what would you have changed?)",Autoencoder Training Experiments (How much effort did they put into tuning the autoencoder hyperparameters?),Autoencoder Architecture Modification (How meaningful was the change in the architecture?),Description of Autoencoder Implementation (Discuss the autoencoder section of their report. How well did they document and describe their training procedure? Did their decisions seem reasonable? Are you convinced that their final model did a good job of encoding the data?),"Model 1 Implementation
(Discuss the model implementation. How much effort did they put into model tuning? Did they pick a model that was appropriate for the task? Did they do anything clever?)","Model 1 Justification
(Discuss how well they justified the choices made while implementing the model.)","Model 2 Implementation
(Discuss the model implementation. How much effort did they put into model tuning? Did they pick a model that was appropriate for the task? Did they do anything clever?)","Model 2 Justification
(Discuss how well they justified the choices made while implementing the model.)","Model 3 Implementation
(Discuss the model implementation. How much effort did they put into model tuning? Did they pick a model that was appropriate for the task? Did they do anything clever?)","Model 3 Justification
(Discuss how well they justified the choices made while implementing the model.)",Discuss the stability check (Was the check relevant to the final model performance? Did it help to convince you that the model would perform well in new situations or that a different data scientist would have produced similar results?),Discuss the final model evaluation (Were you convinced that their final model was the best of the three and that it properly evaluated? Was their process clearly described?),Additional comments/feedback?
7,"run.sh, environment.yaml, lab2.tex or lab2.ipynb, lab2.pdf, Autoencoder checkpoint file in a `results` directory.",None of the above,"Sufficient comments are included to understand code., Consistent style is used.",It's easy to reproduce.,I need to make some changes to run the code as the random forest code is commented and they mentioned that I need to uncomment to run it. ,figures are bit small to read but the document flow is good. ,"Usually, we add very little noise to check the stability of the model and the model is supposed to perform good enough but the report figure showed that accuracy dropped drastically. 
Other graphs look good.",Good. All the figures are labelled. It's hard for me to understand what does the gradient value mean for figure 13.,3,3,"They said they tried 9 different embeddings are tried but the validation loss figure shows only 8 embeddings. Training process discussed is all about hyperparameters. Yes, I think there decision is reasonable and they made a good job at encoding the data.",4,Good,3,Good ,3,They discussed pros and cons of the LightGBM but they didn't discuss why they are using this model.,They added too much noise so the model accuracy changed much. Other than that I am convienced.,"Yes, the results are very good with the model they choose at the end.",
7,"run.sh, environment.yaml, lab2.tex or lab2.ipynb, Autoencoder checkpoint file in a `results` directory.",None of the above,"Sufficient comments are included to understand code., Consistent style is used.",yes,easy to use. The over all structure may be a little bit unclear ,report DNE,"figures are relevant, report DNE",Each to understand with enough notations. Color selection can be better,2,2,report DNE,3,"report DNE, implemented well",3,"report DNE, implemented well",3,"report DNE, implemented well. I can't see their report. They put a lot effort in each model according to their code, with detailed comparison and contrast.","yes, they added noise to 10% of data and results remained","They compared performances in multiple ways in the code, but I'm not sure if it's convincing enough because i can't see their discussion",
7,"run.sh, environment.yaml, lab2.tex or lab2.ipynb, lab2.pdf, Autoencoder checkpoint file in a `results` directory.",None of the above,"Sufficient comments are included to understand code., Sufficient docstrings are included to understand APIs and functions., Consistent style is used.",yes,"Overall, the code appears to be well-organized and modular",The narrative of the report is very clear and easy to follow. ,The figures are very well chosen and highly relevant to the report’s narrative.,The figures in the report are of high quality and contribute significantly to the narrative. ,3,3,The autoencoder section is well-documented and demonstrates a thoughtful approach. ,4,"the rationale is well-articulated, linking both empirical evidence and methodological considerations to the implementation decision.",4,"While KNN’s overall performance was lower than that of the tree-based models, the rationale behind its inclusion was well-supported by its simplicity and the comparative analysis across folds, highlighting both its strengths and its limitations.",4,The rationale for implementing LightGBM is thorough and grounded in empirical evaluations throughout the study,"The stability check not only reinforces the generalizability of the final model but also highlights areas for potential refinement, thus providing a well-rounded assessment of the model’s performance in new situations.",The evaluation process is methodically presented and effectively justifies the selection of the final model,
7,"run.sh, environment.yaml, lab2.tex or lab2.ipynb, lab2.pdf, Autoencoder checkpoint file in a `results` directory.",None of the above,"Sufficient comments are included to understand code., Consistent style is used.","The report seems reproducible : all the figures are in a ""figures"" folder and they seem to match the outputs of jupyter cells.",there is a run.sh file that seems consistent. The overall code structure is consistent with two jupyter notebooks : one for eda and one for modelling. ,"The report is readable, no grammar mistakes","The figures are very relevant, however for half of the figures, there is no explicit reference to them when discussing them.","Figures are easy to understand, the axes are labeled and visually appealing, the colors are not the standard ones. There are captions but they could be more detailed.",3,3,"Their final model is a model with fully connected layers and 40 output dimension. It is surprising that the CNN model is less performant than the fully connected, it could have been more helpful if more details about the number of layers and parameters of the models were given in the report. ",4,The first model is a Random Forest. They fine-tuned the hyperparameters extensively through cross-validation and justified their final choice of model.,4,"The second model is a k-NN. c of model by saying that  this model is attractive because, due to its heuristic nature, it uses very few assumptions.",4,The last model is LGBM.  They fine-tuned the hyperparameters extensively through cross-validation,"the stability check was done by adding noise to the data. they concluded that errors propagate during the process of generating features while incorporating information from surrounding data. This indicates that a data scientist could have produced different results if in his data collection, he collects very noisy data. ","The final model used is LGBM which makes because it is a more powerful model than just random forest or k-NN. The process of selection was very clear, based on cross-validation. The stability and sanity checks were convicing and extensive.",
