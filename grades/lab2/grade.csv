Student Name,Group Number,Email,GitHub,Instructor Notes,run.sh,environment.yaml,lab2.tex/lab2.ipynb,lab2.pdf,Results dir,Anonymous,Page Limit,Has Code,Data Included,Comments,Docstrings,Style,Hard-Coded Paths,Clear Instructions,Figures (2pts),Descriptions (3pts),Feature Selection (3pts),New Features (2pts),Training (3pts),Description (2pts),Architecture Modification (3pts),Evaluation (2pts),Model #1 (4pts),Model #2 (4pts),Model #3 (4pts),Explanation (8pts),Model Fit (2pts),Justification for Best Classifier (3pts),Relevance of Post-Hoc EDA (2pts),Relevance (3pts),Readability (3pts),Quality of Figures (2pts),Points,Percentage
Tristan Erz,7,erztristan@berkeley.edu,erztristan,"EDA: Good work, especially with the outlier analysis. Feature Engineering: Usage of CART for feature importance is a good idea, but the analysis isn't very quantitative and could be made more robust by looking at a random forest model with MDI. Modeling: Well done, overall. The confusion matrices are great, but it would have been helpful to have a table at the end comparing metrics other than accuracy accross the models (e.g. F1 score, sensitivity, specificity, etc.). Stability Check: Should include a perturbation in one of the judgement calls you made during the modeling pipeline (other than data).",1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0,0.0,1.0,1.0,1.0,1.0,0.0,2.0,3.0,1.5,2.0,3.0,2.0,3.0,2.0,4.0,4.0,4.0,8.0,2.0,2.0,2.0,2.0,3.0,2.0,60.5,93.08
